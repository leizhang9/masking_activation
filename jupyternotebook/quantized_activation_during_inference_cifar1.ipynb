{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8672cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model without any quantization is trained, then run inference with the model with quantized activation function \n",
    "# for different quantization levels, and compare their accuracy.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6652f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_x:(50000, 32, 32, 3), train_y:(50000, 1), test_x:(10000, 32, 32, 3), test_y:(10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 14:55:44.555713: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 72.0KiB (rounded to 73728)requested by op AddV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-06-04 14:55:44.555746: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2024-06-04 14:55:44.555758: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 37, Chunks in use: 37. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 3.3KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555766: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555774: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555783: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 4, Chunks in use: 3. 13.0KiB allocated for chunks. 10.5KiB in use in bin. 10.1KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555792: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 2, Chunks in use: 1. 12.8KiB allocated for chunks. 6.2KiB in use in bin. 3.4KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555801: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 13.5KiB allocated for chunks. 13.5KiB in use in bin. 9.8KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555809: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555817: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 49.0KiB allocated for chunks. 49.0KiB in use in bin. 48.8KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555827: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 5, Chunks in use: 5. 401.8KiB allocated for chunks. 401.8KiB in use in bin. 360.0KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555835: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555844: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 3, Chunks in use: 3. 864.0KiB allocated for chunks. 864.0KiB in use in bin. 864.0KiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555851: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555859: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555867: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555874: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555884: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 12.10MiB allocated for chunks. 12.10MiB in use in bin. 9.00MiB client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555892: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555899: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555907: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555928: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-04 14:55:44.555952: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 72.0KiB was 64.0KiB, Chunk State: \n",
      "2024-06-04 14:55:44.555964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 14090240\n",
      "2024-06-04 14:55:44.555976: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000000 of size 256 next 4\n",
      "2024-06-04 14:55:44.555986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000100 of size 256 next 6\n",
      "2024-06-04 14:55:44.555994: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000200 of size 256 next 7\n",
      "2024-06-04 14:55:44.556001: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000300 of size 256 next 8\n",
      "2024-06-04 14:55:44.556008: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000400 of size 256 next 9\n",
      "2024-06-04 14:55:44.556015: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000500 of size 256 next 12\n",
      "2024-06-04 14:55:44.556021: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000600 of size 256 next 13\n",
      "2024-06-04 14:55:44.556028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000700 of size 256 next 14\n",
      "2024-06-04 14:55:44.556034: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000800 of size 256 next 15\n",
      "2024-06-04 14:55:44.556041: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000900 of size 256 next 16\n",
      "2024-06-04 14:55:44.556048: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000a00 of size 256 next 17\n",
      "2024-06-04 14:55:44.556055: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000b00 of size 256 next 18\n",
      "2024-06-04 14:55:44.556062: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000c00 of size 256 next 19\n",
      "2024-06-04 14:55:44.556069: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000d00 of size 256 next 22\n",
      "2024-06-04 14:55:44.556078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000e00 of size 256 next 23\n",
      "2024-06-04 14:55:44.556085: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2000f00 of size 512 next 24\n",
      "2024-06-04 14:55:44.556093: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001100 of size 256 next 27\n",
      "2024-06-04 14:55:44.556100: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001200 of size 256 next 28\n",
      "2024-06-04 14:55:44.556107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001300 of size 256 next 29\n",
      "2024-06-04 14:55:44.556114: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001400 of size 256 next 3\n",
      "2024-06-04 14:55:44.556120: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001500 of size 256 next 34\n",
      "2024-06-04 14:55:44.556127: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001600 of size 256 next 35\n",
      "2024-06-04 14:55:44.556134: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001700 of size 256 next 36\n",
      "2024-06-04 14:55:44.556140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001800 of size 256 next 37\n",
      "2024-06-04 14:55:44.556147: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001900 of size 256 next 40\n",
      "2024-06-04 14:55:44.556154: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001a00 of size 256 next 41\n",
      "2024-06-04 14:55:44.556161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001b00 of size 256 next 42\n",
      "2024-06-04 14:55:44.556168: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001c00 of size 256 next 5\n",
      "2024-06-04 14:55:44.556175: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001d00 of size 256 next 31\n",
      "2024-06-04 14:55:44.556183: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001e00 of size 256 next 44\n",
      "2024-06-04 14:55:44.556189: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2001f00 of size 256 next 10\n",
      "2024-06-04 14:55:44.556196: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2002000 of size 3584 next 11\n",
      "2024-06-04 14:55:44.556203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2002e00 of size 256 next 45\n",
      "2024-06-04 14:55:44.556210: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2002f00 of size 256 next 46\n",
      "2024-06-04 14:55:44.556217: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efad2003000 of size 6656 next 33\n",
      "2024-06-04 14:55:44.556224: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2004a00 of size 3584 next 30\n",
      "2024-06-04 14:55:44.556231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2005800 of size 256 next 50\n",
      "2024-06-04 14:55:44.556239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2005900 of size 256 next 52\n",
      "2024-06-04 14:55:44.556253: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2005a00 of size 256 next 53\n",
      "2024-06-04 14:55:44.556264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2005b00 of size 256 next 54\n",
      "2024-06-04 14:55:44.556273: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2005c00 of size 256 next 55\n",
      "2024-06-04 14:55:44.556288: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efad2005d00 of size 2560 next 51\n",
      "2024-06-04 14:55:44.556298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2006700 of size 6400 next 43\n",
      "2024-06-04 14:55:44.556305: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2008000 of size 3584 next 32\n",
      "2024-06-04 14:55:44.556313: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2008e00 of size 13824 next 1\n",
      "2024-06-04 14:55:44.556326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad200c400 of size 1280 next 2\n",
      "2024-06-04 14:55:44.556343: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad200c900 of size 73728 next 39\n",
      "2024-06-04 14:55:44.556351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad201e900 of size 73728 next 38\n",
      "2024-06-04 14:55:44.556360: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2030900 of size 50176 next 49\n",
      "2024-06-04 14:55:44.556371: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad203cd00 of size 97280 next 48\n",
      "2024-06-04 14:55:44.556385: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2054900 of size 92928 next 21\n",
      "2024-06-04 14:55:44.556392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad206b400 of size 73728 next 20\n",
      "2024-06-04 14:55:44.556399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad207d400 of size 294912 next 47\n",
      "2024-06-04 14:55:44.556410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad20c5400 of size 294912 next 26\n",
      "2024-06-04 14:55:44.556426: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad210d400 of size 294912 next 25\n",
      "2024-06-04 14:55:44.556434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efad2155400 of size 12692480 next 18446744073709551615\n",
      "2024-06-04 14:55:44.556441: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2024-06-04 14:55:44.556451: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 37 Chunks of size 256 totalling 9.2KiB\n",
      "2024-06-04 14:55:44.556467: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 512 totalling 512B\n",
      "2024-06-04 14:55:44.556480: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-06-04 14:55:44.556492: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2024-06-04 14:55:44.556504: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 6400 totalling 6.2KiB\n",
      "2024-06-04 14:55:44.556516: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 13824 totalling 13.5KiB\n",
      "2024-06-04 14:55:44.556528: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 50176 totalling 49.0KiB\n",
      "2024-06-04 14:55:44.556542: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 73728 totalling 216.0KiB\n",
      "2024-06-04 14:55:44.556554: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 92928 totalling 90.8KiB\n",
      "2024-06-04 14:55:44.556567: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 97280 totalling 95.0KiB\n",
      "2024-06-04 14:55:44.556579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 294912 totalling 864.0KiB\n",
      "2024-06-04 14:55:44.556595: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 12692480 totalling 12.10MiB\n",
      "2024-06-04 14:55:44.556608: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 13.43MiB\n",
      "2024-06-04 14:55:44.556622: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 14090240 memory_limit_: 14090240 available bytes: 0 curr_region_allocation_bytes_: 28180480\n",
      "2024-06-04 14:55:44.556642: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                        14090240\n",
      "InUse:                        14081024\n",
      "MaxInUse:                     14081280\n",
      "NumAllocs:                          90\n",
      "MaxAllocSize:                 12692480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-06-04 14:55:44.556659: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *****************************************************************************xxxxxxxxxxxxxxxxxxxxxxx\n",
      "2024-06-04 14:55:44.556676: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "failed to allocate memory [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the model without quantization\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m():\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Convolutional layer 1\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Convolutional layer 2\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# # Convolutional layer 3\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Conv2D(128, (3, 3), activation='relu'),\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dropout(0.4),\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Fully connected layer 1\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Output layer\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n",
      "File \u001b[0;32m/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/keras/backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator:\n\u001b[1;32m   1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   1919\u001b[0m       shape\u001b[38;5;241m=\u001b[39mshape, minval\u001b[38;5;241m=\u001b[39mminval, maxval\u001b[38;5;241m=\u001b[39mmaxval, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 1920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_legacy_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_x,train_y),(test_x,test_y) = cifar10.load_data()\n",
    "print('\\n train_x:%s, train_y:%s, test_x:%s, test_y:%s'%(train_x.shape,train_y.shape,test_x.shape,test_y.shape)) \n",
    "\n",
    "x_train, x_test = train_x / 255.0, test_x / 255.0  # Normalize to [0,1]\n",
    "y_train,y_test = tf.cast(train_y,tf.int8),tf.cast(test_y,tf.int8)\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # # Convolutional layer 3\n",
    "    # Conv2D(128, (3, 3), activation='relu'),\n",
    "    # Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbfd74-e953-4d27-bf49-0242877ffb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816782a-a052-4f92-abf2-a1207f7b7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def quantized_relu(x, levels):\n",
    "    x = tf.nn.relu(x)\n",
    "    max_val = tf.reduce_max(x)\n",
    "    # Normalize the clipped output to [0, 1] for quantization\n",
    "    x_normalized = x / max_val\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [0, max_val]\n",
    "    x_scaled_back = x_quantized * max_val\n",
    "    return x_scaled_back\n",
    "    \n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8e1c9-0eea-4f26-805c-4d1c5307ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "quantization_levels = [4, 8, 16, 32]\n",
    "accuracy_list = []\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=lambda x: quantized_relu(x, levels), input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "    ])\n",
    "\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    # Compile the model\n",
    "    new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    \n",
    "    \n",
    "    # Generate representative dataset for quantization\n",
    "    def representative_dataset_gen():\n",
    "        for _ in range(100):\n",
    "            # Get sample input data as a numpy array in a method of your choosing\n",
    "            yield [x_train[np.random.randint(x_train.shape[0], size=1)]]\n",
    "\n",
    "    # Convert the model to a TensorFlow Lite model with quantization\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset_gen\n",
    "    # Ensure that if any ops can't be quantized, they are converted to float32 instead of failing\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # Ensure the input and output tensors are also quantized to int8\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    # Save the quantized model in the current directory\n",
    "    model_path = os.path.join(current_directory, f'quantized_mnist_model_{levels}levels.tflite')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"Quantized model saved to: {model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=f'quantized_mnist_model_{levels}levels.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # Convert test data to uint8\n",
    "    x_test_uint8 = (x_test * 255).astype(np.uint8)\n",
    "    # Function to run inference on a single image\n",
    "    def run_inference(image):\n",
    "        interpreter.set_tensor(input_details[0]['index'], image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        return np.argmax(output)\n",
    "    # Compute accuracy on the test dataset\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i in range(len(x_test_uint8)):\n",
    "        image = np.expand_dims(x_test_uint8[i], axis=0)\n",
    "        label = y_test[i]\n",
    "        prediction = run_inference(image)\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72969f14-8f26-44c9-8507-5b903e7af353",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for levels in quantization_levels:\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {accuracy_list[i] * 100:.2f}%\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61030793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

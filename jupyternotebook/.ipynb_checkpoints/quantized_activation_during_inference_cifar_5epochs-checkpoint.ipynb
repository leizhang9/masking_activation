{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model without any quantization is trained, then run inference with the model with quantized activation function \n",
    "# for different quantization levels, and compare their accuracy.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_x:(50000, 32, 32, 3), train_y:(50000, 1), test_x:(10000, 32, 32, 3), test_y:(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_x,train_y),(test_x,test_y) = cifar10.load_data()\n",
    "print('\\n train_x:%s, train_y:%s, test_x:%s, test_y:%s'%(train_x.shape,train_y.shape,test_x.shape,test_y.shape)) \n",
    "\n",
    "x_train, x_test = train_x / 255.0, test_x / 255.0  # Normalize to [0,1]\n",
    "y_train,y_test = tf.cast(train_y,tf.int16),tf.cast(test_y,tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 107s 75ms/step - loss: 1.5047 - sparse_categorical_accuracy: 0.4624 - val_loss: 1.1638 - val_sparse_categorical_accuracy: 0.5918\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 119s 85ms/step - loss: 1.1372 - sparse_categorical_accuracy: 0.5989 - val_loss: 0.9478 - val_sparse_categorical_accuracy: 0.6748\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 122s 86ms/step - loss: 0.9940 - sparse_categorical_accuracy: 0.6528 - val_loss: 0.9771 - val_sparse_categorical_accuracy: 0.6530\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 121s 86ms/step - loss: 0.9132 - sparse_categorical_accuracy: 0.6802 - val_loss: 0.8957 - val_sparse_categorical_accuracy: 0.6888\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 120s 86ms/step - loss: 0.8442 - sparse_categorical_accuracy: 0.7047 - val_loss: 0.8341 - val_sparse_categorical_accuracy: 0.7166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2193504ccf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "# # Visualize the model\n",
    "# model.summary()\n",
    "# print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, 'activation'):\n",
    "#         activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#     else:\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.7074\n",
      "Quantized Model Accuracy with Quantized relu(4 levels): 0.2191, Loss of Accuracy: 69.03%\n",
      "Quantized Model Accuracy with Quantized relu(8 levels): 0.5139, Loss of Accuracy: 27.35%\n",
      "Quantized Model Accuracy with Quantized relu(16 levels): 0.6904, Loss of Accuracy: 2.40%\n",
      "Quantized Model Accuracy with Quantized relu(32 levels): 0.7052, Loss of Accuracy: 0.31%\n",
      "Quantized Model Accuracy with Quantized relu(64 levels): 0.7076, Loss of Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# layer_outputs_model = tf.keras.Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "# test_data = x_test[0:1]\n",
    "# layer_activations = layer_outputs_model.predict(test_data)\n",
    "# print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "# print(\"layer_activations: \", layer_activations)\n",
    "\n",
    "def quantized_relu(x, levels):\n",
    "    x = tf.nn.relu(x)\n",
    "    max_val = tf.reduce_max(x)\n",
    "    # Normalize the clipped output to [0, 1] for quantization\n",
    "    x_normalized = x / max_val\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [0, max_val]\n",
    "    x_scaled_back = x_quantized * max_val\n",
    "    return x_scaled_back\n",
    "\n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized\n",
    "\n",
    "quantization_levels = [4, 8, 16, 32, 64]\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=lambda x: quantized_relu(x, levels), input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "])\n",
    "\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    # Compile the model\n",
    "    new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "    \n",
    "    \n",
    "    # visualize the quantized model\n",
    "#     new_model.summary()\n",
    "#     print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "#     for layer in new_model.layers:\n",
    "#         if hasattr(layer, 'activation'):\n",
    "#             activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#         else:\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: None\")\n",
    "\n",
    "#     Evaluate the model with quantized activation function during inference\n",
    "    quantized_accuracy = new_model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "    accuracy_loss = 0 if quantized_accuracy > original_accuracy else ((original_accuracy - quantized_accuracy) / original_accuracy) * 100\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {quantized_accuracy:.4f}, Loss of Accuracy: {accuracy_loss:.2f}%\")\n",
    "    \n",
    "#     # Verify the layer outputs are quantized\n",
    "#     layer_outputs_model = tf.keras.Model(inputs=new_model.input, outputs=new_model.layers[2].output)\n",
    "#     test_data = x_test[0:1]\n",
    "#     layer_activations = layer_outputs_model.predict(test_data)\n",
    "#     print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "#     print(\"layer_activations: \", layer_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 116s 82ms/step - loss: 2.0362 - sparse_categorical_accuracy: 0.2865 - val_loss: 2.3267 - val_sparse_categorical_accuracy: 0.1994\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 131s 93ms/step - loss: 1.7161 - sparse_categorical_accuracy: 0.3766 - val_loss: 1.9801 - val_sparse_categorical_accuracy: 0.2972\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 136s 97ms/step - loss: 1.6166 - sparse_categorical_accuracy: 0.4138 - val_loss: 1.5038 - val_sparse_categorical_accuracy: 0.4478\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 138s 98ms/step - loss: 1.5453 - sparse_categorical_accuracy: 0.4374 - val_loss: 1.5427 - val_sparse_categorical_accuracy: 0.4428\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 134s 95ms/step - loss: 1.5022 - sparse_categorical_accuracy: 0.4533 - val_loss: 1.3987 - val_sparse_categorical_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21936231c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantization of tanh\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='tanh', input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='tanh'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation='tanh'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "# # Visualize the model\n",
    "# model.summary()\n",
    "# print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, 'activation'):\n",
    "#         activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#     else:\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.4885\n",
      "Quantized Model Accuracy with Quantized tanh(4 levels): 0.3144, Loss of Accuracy: 35.64%\n",
      "Quantized Model Accuracy with Quantized tanh(8 levels): 0.4696, Loss of Accuracy: 3.87%\n",
      "Quantized Model Accuracy with Quantized tanh(16 levels): 0.4772, Loss of Accuracy: 2.31%\n",
      "Quantized Model Accuracy with Quantized tanh(32 levels): 0.4873, Loss of Accuracy: 0.25%\n",
      "Quantized Model Accuracy with Quantized tanh(64 levels): 0.4881, Loss of Accuracy: 0.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# Quantization of tanh\n",
    "def quantized_tanh(x, levels):\n",
    "    # Apply the modified tanh activation\n",
    "    x = 1 - 2 / (tf.math.exp(2 * x) + 1)\n",
    "    # Normalize the activation output to [0, 1] to prepare for quantization\n",
    "    x_normalized = (x + 1) / 2\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [-1, 1]\n",
    "    x_scaled_back = x_quantized * 2 - 1\n",
    "    return x_scaled_back\n",
    "\n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized\n",
    "\n",
    "quantization_levels = [4, 8, 16, 32, 64]\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=lambda x: quantized_tanh(x, levels), input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation=lambda x: quantized_tanh(x, levels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation=lambda x: quantized_tanh(x, levels)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation=lambda x: quantized_tanh(x, levels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "    ])\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    # Compile the model\n",
    "    new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "    \n",
    "#     # visualize the quantized model\n",
    "#     new_model.summary()\n",
    "#     print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "#     for layer in new_model.layers:\n",
    "#         if hasattr(layer, 'activation'):\n",
    "#             activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#         else:\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: None\")\n",
    "\n",
    "    # Evaluate the model with quantized activation function during inference\n",
    "    quantized_accuracy = new_model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "    accuracy_loss = 0 if quantized_accuracy > original_accuracy else ((original_accuracy - quantized_accuracy) / original_accuracy) * 100\n",
    "    print(f\"Quantized Model Accuracy with Quantized tanh({levels} levels): {quantized_accuracy:.4f}, Loss of Accuracy: {accuracy_loss:.2f}%\")\n",
    "    \n",
    "#     # Verify the layer outputs are quantized\n",
    "#     layer_outputs_model = tf.keras.Model(inputs=new_model.input, outputs=new_model.layers[2].output)\n",
    "#     test_data = x_test[0:1]\n",
    "#     layer_activations = layer_outputs_model.predict(test_data)\n",
    "#     print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "#     print(\"layer_activations: \", layer_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

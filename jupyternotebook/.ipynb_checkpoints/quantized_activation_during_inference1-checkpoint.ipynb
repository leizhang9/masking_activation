{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8672cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model without any quantization is trained, then run inference with the model with quantized activation function \n",
    "# for different quantization levels, and compare their accuracy.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6652f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2166 - accuracy: 0.9368 - val_loss: 0.0934 - val_accuracy: 0.9732\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0876 - accuracy: 0.9726 - val_loss: 0.0748 - val_accuracy: 0.9742\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0621 - accuracy: 0.9797 - val_loss: 0.0774 - val_accuracy: 0.9775\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0430 - accuracy: 0.9861 - val_loss: 0.0846 - val_accuracy: 0.9770\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0823 - val_accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbe4435f40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset:mnist, quantization of relu\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0 , x_test/255.0   # Normalize to [0,1]\n",
    "\n",
    "# Ensure the data is of type float32\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbfd74-e953-4d27-bf49-0242877ffb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4816782a-a052-4f92-abf2-a1207f7b7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def quantized_relu(x, levels):\n",
    "    x = tf.nn.relu(x)\n",
    "    max_val = tf.reduce_max(x)\n",
    "    # Normalize the clipped output to [0, 1] for quantization\n",
    "    x_normalized = x / max_val\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [0, max_val]\n",
    "    x_scaled_back = x_quantized * max_val\n",
    "    return x_scaled_back\n",
    "    \n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ef8e1c9-0eea-4f26-805c-4d1c5307ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpr7niz7vy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpr7niz7vy/assets\n",
      "/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2024-06-04 14:32:19.313307: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-06-04 14:32:19.313332: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-06-04 14:32:19.313514: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpr7niz7vy\n",
      "2024-06-04 14:32:19.314510: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-06-04 14:32:19.314524: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpr7niz7vy\n",
      "2024-06-04 14:32:19.317358: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-06-04 14:32:19.332843: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpr7niz7vy\n",
      "2024-06-04 14:32:19.340685: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 27172 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: /nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/jupyternotebook/quantized_mnist_model_4levels.tflite\n",
      "Quantized Model Accuracy with Quantized relu(4 levels): 96.82%\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpplwjz9jk/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpplwjz9jk/assets\n",
      "/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2024-06-04 14:32:21.745904: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-06-04 14:32:21.745929: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-06-04 14:32:21.746107: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpplwjz9jk\n",
      "2024-06-04 14:32:21.747141: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-06-04 14:32:21.747155: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpplwjz9jk\n",
      "2024-06-04 14:32:21.750047: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-06-04 14:32:21.765295: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpplwjz9jk\n",
      "2024-06-04 14:32:21.773393: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 27287 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: /nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/jupyternotebook/quantized_mnist_model_8levels.tflite\n",
      "Quantized Model Accuracy with Quantized relu(8 levels): 97.33%\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpm7xdlif8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm7xdlif8/assets\n",
      "/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2024-06-04 14:32:24.004381: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-06-04 14:32:24.004405: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-06-04 14:32:24.004584: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpm7xdlif8\n",
      "2024-06-04 14:32:24.005549: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-06-04 14:32:24.005563: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpm7xdlif8\n",
      "2024-06-04 14:32:24.008298: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-06-04 14:32:24.023112: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpm7xdlif8\n",
      "2024-06-04 14:32:24.030871: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 26288 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: /nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/jupyternotebook/quantized_mnist_model_16levels.tflite\n",
      "Quantized Model Accuracy with Quantized relu(16 levels): 97.38%\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpfdtj936a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpfdtj936a/assets\n",
      "/nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/Attack-Framework/attack_python/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2024-06-04 14:32:26.213952: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-06-04 14:32:26.213974: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-06-04 14:32:26.214169: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpfdtj936a\n",
      "2024-06-04 14:32:26.215146: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-06-04 14:32:26.215160: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpfdtj936a\n",
      "2024-06-04 14:32:26.217784: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-06-04 14:32:26.232817: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpfdtj936a\n",
      "2024-06-04 14:32:26.240437: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 26268 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: /nas/ei/home/ge36cig/Desktop/ma_zhang_masking_activation_functions_for_nns/jupyternotebook/quantized_mnist_model_32levels.tflite\n",
      "Quantized Model Accuracy with Quantized relu(32 levels): 97.39%\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "quantization_levels = [4, 8, 16, 32]\n",
    "accuracy_list = []\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(300, activation=lambda x: quantized_relu(x, levels)),\n",
    "        Dense(100, activation=lambda x: quantized_relu(x, levels)),\n",
    "        Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "    ])\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    new_model.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Generate representative dataset for quantization\n",
    "    def representative_dataset_gen():\n",
    "        for _ in range(100):\n",
    "            # Get sample input data as a numpy array in a method of your choosing\n",
    "            yield [x_train[np.random.randint(x_train.shape[0], size=1)]]\n",
    "\n",
    "    # Convert the model to a TensorFlow Lite model with quantization\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset_gen\n",
    "    # Ensure that if any ops can't be quantized, they are converted to float32 instead of failing\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # Ensure the input and output tensors are also quantized to int8\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    # Save the quantized model in the current directory\n",
    "    model_path = os.path.join(current_directory, f'quantized_mnist_model_{levels}levels.tflite')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"Quantized model saved to: {model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=f'quantized_mnist_model_{levels}levels.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # Convert test data to uint8\n",
    "    x_test_uint8 = (x_test * 255).astype(np.uint8)\n",
    "    # Function to run inference on a single image\n",
    "    def run_inference(image):\n",
    "        interpreter.set_tensor(input_details[0]['index'], image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        return np.argmax(output)\n",
    "    # Compute accuracy on the test dataset\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i in range(len(x_test_uint8)):\n",
    "        image = np.expand_dims(x_test_uint8[i], axis=0)\n",
    "        label = y_test[i]\n",
    "        prediction = run_inference(image)\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72969f14-8f26-44c9-8507-5b903e7af353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Accuracy with Quantized relu(4 levels): 96.82%\n",
      "Quantized Model Accuracy with Quantized relu(8 levels): 97.33%\n",
      "Quantized Model Accuracy with Quantized relu(16 levels): 97.38%\n",
      "Quantized Model Accuracy with Quantized relu(32 levels): 97.39%\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for levels in quantization_levels:\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {accuracy_list[i] * 100:.2f}%\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61030793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model without any quantization is trained, then run inference with the model with quantized activation function \n",
    "# for different quantization levels, and compare their accuracy.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_x:(50000, 32, 32, 3), train_y:(50000, 1), test_x:(10000, 32, 32, 3), test_y:(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_x,train_y),(test_x,test_y) = cifar10.load_data()\n",
    "print('\\n train_x:%s, train_y:%s, test_x:%s, test_y:%s'%(train_x.shape,train_y.shape,test_x.shape,test_y.shape)) \n",
    "\n",
    "x_train, x_test = train_x / 255.0, test_x / 255.0  # Normalize to [0,1]\n",
    "y_train,y_test = tf.cast(train_y,tf.int16),tf.cast(test_y,tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 106s 75ms/step - loss: 1.5319 - sparse_categorical_accuracy: 0.4483 - val_loss: 1.1454 - val_sparse_categorical_accuracy: 0.5926\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 119s 84ms/step - loss: 1.1618 - sparse_categorical_accuracy: 0.5932 - val_loss: 1.3486 - val_sparse_categorical_accuracy: 0.5474\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 121s 86ms/step - loss: 1.0108 - sparse_categorical_accuracy: 0.6503 - val_loss: 1.0267 - val_sparse_categorical_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 122s 87ms/step - loss: 0.9173 - sparse_categorical_accuracy: 0.6790 - val_loss: 0.8627 - val_sparse_categorical_accuracy: 0.7094\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 122s 87ms/step - loss: 0.8521 - sparse_categorical_accuracy: 0.7022 - val_loss: 0.8848 - val_sparse_categorical_accuracy: 0.6998\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 121s 86ms/step - loss: 0.7867 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.7773 - val_sparse_categorical_accuracy: 0.7348\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 121s 86ms/step - loss: 0.7416 - sparse_categorical_accuracy: 0.7395 - val_loss: 0.8809 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 122s 86ms/step - loss: 0.6834 - sparse_categorical_accuracy: 0.7603 - val_loss: 0.7649 - val_sparse_categorical_accuracy: 0.7394\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 122s 87ms/step - loss: 0.6510 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.7591 - val_sparse_categorical_accuracy: 0.7420\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 122s 87ms/step - loss: 0.6046 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.7848 - val_sparse_categorical_accuracy: 0.7372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b75c38e908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# # Visualize the model\n",
    "# model.summary()\n",
    "# print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, 'activation'):\n",
    "#         activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#     else:\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.7282\n",
      "Quantized Model Accuracy with Quantized relu(4 levels): 0.1202, Loss of Accuracy: 83.49%\n",
      "Quantized Model Accuracy with Quantized relu(8 levels): 0.4052, Loss of Accuracy: 44.36%\n",
      "Quantized Model Accuracy with Quantized relu(16 levels): 0.6404, Loss of Accuracy: 12.06%\n",
      "Quantized Model Accuracy with Quantized relu(32 levels): 0.7100, Loss of Accuracy: 2.50%\n",
      "Quantized Model Accuracy with Quantized relu(64 levels): 0.7259, Loss of Accuracy: 0.32%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# layer_outputs_model = tf.keras.Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "# test_data = x_test[0:1]\n",
    "# layer_activations = layer_outputs_model.predict(test_data)\n",
    "# print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "# print(\"layer_activations: \", layer_activations)\n",
    "\n",
    "def quantized_relu(x, levels):\n",
    "    x = tf.nn.relu(x)\n",
    "    max_val = tf.reduce_max(x)\n",
    "    # Normalize the clipped output to [0, 1] for quantization\n",
    "    x_normalized = x / max_val\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [0, max_val]\n",
    "    x_scaled_back = x_quantized * max_val\n",
    "    return x_scaled_back\n",
    "\n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized\n",
    "\n",
    "quantization_levels = [4, 8, 16, 32, 64]\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=lambda x: quantized_relu(x, levels), input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation=lambda x: quantized_relu(x, levels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "])\n",
    "\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    # Compile the model\n",
    "    new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "    \n",
    "    \n",
    "    # visualize the quantized model\n",
    "#     new_model.summary()\n",
    "#     print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "#     for layer in new_model.layers:\n",
    "#         if hasattr(layer, 'activation'):\n",
    "#             activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#         else:\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: None\")\n",
    "\n",
    "#     Evaluate the model with quantized activation function during inference\n",
    "    quantized_accuracy = new_model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "    accuracy_loss = 0 if quantized_accuracy > original_accuracy else ((original_accuracy - quantized_accuracy) / original_accuracy) * 100\n",
    "    print(f\"Quantized Model Accuracy with Quantized relu({levels} levels): {quantized_accuracy:.4f}, Loss of Accuracy: {accuracy_loss:.2f}%\")\n",
    "    \n",
    "#     # Verify the layer outputs are quantized\n",
    "#     layer_outputs_model = tf.keras.Model(inputs=new_model.input, outputs=new_model.layers[2].output)\n",
    "#     test_data = x_test[0:1]\n",
    "#     layer_activations = layer_outputs_model.predict(test_data)\n",
    "#     print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "#     print(\"layer_activations: \", layer_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 127s 90ms/step - loss: 2.0213 - sparse_categorical_accuracy: 0.2917 - val_loss: 1.8062 - val_sparse_categorical_accuracy: 0.3478\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 126s 90ms/step - loss: 1.7193 - sparse_categorical_accuracy: 0.3775 - val_loss: 1.6336 - val_sparse_categorical_accuracy: 0.4194\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 126s 89ms/step - loss: 1.6192 - sparse_categorical_accuracy: 0.4112 - val_loss: 1.6733 - val_sparse_categorical_accuracy: 0.4004\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 125s 89ms/step - loss: 1.5575 - sparse_categorical_accuracy: 0.4347 - val_loss: 1.4377 - val_sparse_categorical_accuracy: 0.4868\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 126s 90ms/step - loss: 1.4954 - sparse_categorical_accuracy: 0.4573 - val_loss: 1.4087 - val_sparse_categorical_accuracy: 0.4956\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 126s 90ms/step - loss: 1.4459 - sparse_categorical_accuracy: 0.4778 - val_loss: 1.4528 - val_sparse_categorical_accuracy: 0.4828\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 126s 89ms/step - loss: 1.4145 - sparse_categorical_accuracy: 0.4912 - val_loss: 1.4642 - val_sparse_categorical_accuracy: 0.4702\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 126s 90ms/step - loss: 1.3918 - sparse_categorical_accuracy: 0.4969 - val_loss: 1.3040 - val_sparse_categorical_accuracy: 0.5358\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 126s 90ms/step - loss: 1.3898 - sparse_categorical_accuracy: 0.5001 - val_loss: 1.5164 - val_sparse_categorical_accuracy: 0.4400\n",
      "Epoch 10/10\n",
      "1136/1407 [=======================>......] - ETA: 23s - loss: 1.3620 - sparse_categorical_accuracy: 0.5137"
     ]
    }
   ],
   "source": [
    "# quantization of tanh\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='tanh', input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='tanh'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation='tanh'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model without quantization\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# # Visualize the model\n",
    "# model.summary()\n",
    "# print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, 'activation'):\n",
    "#         activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#     else:\n",
    "#         print(f\"Layer: {layer.name}, Activation Function: None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model without quantized activation function during inference\n",
    "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# Quantization of tanh\n",
    "def quantized_tanh(x, levels):\n",
    "    # Apply the modified tanh activation\n",
    "    x = 1 - 2 / (tf.math.exp(2 * x) + 1)\n",
    "    # Normalize the activation output to [0, 1] to prepare for quantization\n",
    "    x_normalized = (x + 1) / 2\n",
    "    # Quantize the normalized output\n",
    "    x_quantized = tf.round(x_normalized * (levels - 1)) / (levels - 1)\n",
    "    # Scale back to [-1, 1]\n",
    "    x_scaled_back = x_quantized * 2 - 1\n",
    "    return x_scaled_back\n",
    "\n",
    "def quantized_softmax(x, levels):\n",
    "    x_softmax = tf.nn.softmax(x)\n",
    "    # Since softmax outputs are already in [0, 1], we can quantize them directly\n",
    "    x_quantized = tf.round(x_softmax * (levels - 1)) / (levels - 1)\n",
    "    return x_quantized\n",
    "\n",
    "quantization_levels = [4, 8, 16, 32, 64]\n",
    "\n",
    "for levels in quantization_levels:\n",
    "    new_model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=lambda x: quantized_tanh(x, levels), input_shape=x_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation=lambda x: quantized_tanh(x, levels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (3, 3), activation=lambda x: quantized_tanh(x, levels)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    Dense(128, activation=lambda x: quantized_tanh(x, levels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(10, activation=lambda x: quantized_softmax(x, levels))\n",
    "    ])\n",
    "    # get trained weights\n",
    "    for layer, new_layer in zip(model.layers, new_model.layers):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        \n",
    "    # Compile the model\n",
    "    new_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy']) \n",
    "    \n",
    "#     # visualize the quantized model\n",
    "#     new_model.summary()\n",
    "#     print(\"\\nDetailed Summary with Activation Functions:\")\n",
    "#     for layer in new_model.layers:\n",
    "#         if hasattr(layer, 'activation'):\n",
    "#             activation = layer.activation.__name__ if layer.activation else 'None'\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: {activation}\")\n",
    "#         else:\n",
    "#             print(f\"Layer: {layer.name}, Activation Function: None\")\n",
    "\n",
    "    # Evaluate the model with quantized activation function during inference\n",
    "    quantized_accuracy = new_model.evaluate(x_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "    accuracy_loss = 0 if quantized_accuracy > original_accuracy else ((original_accuracy - quantized_accuracy) / original_accuracy) * 100\n",
    "    print(f\"Quantized Model Accuracy with Quantized tanh({levels} levels): {quantized_accuracy:.4f}, Loss of Accuracy: {accuracy_loss:.2f}%\")\n",
    "    \n",
    "#     # Verify the layer outputs are quantized\n",
    "#     layer_outputs_model = tf.keras.Model(inputs=new_model.input, outputs=new_model.layers[2].output)\n",
    "#     test_data = x_test[0:1]\n",
    "#     layer_activations = layer_outputs_model.predict(test_data)\n",
    "#     print(\"shape of layer_activations: \", layer_activations.shape)\n",
    "#     print(\"layer_activations: \", layer_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
